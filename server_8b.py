import os
import socket
from fastapi import FastAPI, Request
from fastapi.responses import Response, JSONResponse
from vllm import LLM, SamplingParams
import uvicorn
import gc
import torch

node_ip = socket.gethostbyname(socket.gethostname())
num_gpus = torch.cuda.device_count()
app = FastAPI()

ip_path = "~/state_entropy_decode/src/sal/reward_ip.txt"
ip_path = os.path.expanduser(ip_path)

with open(ip_path, "w") as f:
    f.write(node_ip)
print(f"reward server running on: {node_ip}")

llm = LLM( # 14GB
    model='RLHFlow/Llama3.1-8B-PRM-Deepseek-Data',
    tensor_parallel_size=num_gpus,
    max_model_len = 48240,
    gpu_memory_utilization=0.55
)

tokenizer = llm.get_tokenizer()

sampling_params = SamplingParams(
    logprobs=20,
    prompt_logprobs=20,
)

good_token = '+'
bad_token = '-'
step_tag = 'ки'

plus_tag_id = tokenizer.encode(good_token)[-1]
minus_tag_id = tokenizer.encode(bad_token)[-1]
step_tag_id = tokenizer.encode(step_tag)[-1]


def free_memory():
    gc.collect()
    torch.cuda.empty_cache()
    torch.cuda.ipc_collect()

def softmax_from_logprob(probs):
    """
    Calculate softmax probabilities from log probabilities using PyTorch.
    
    Args:
        log_probs (torch.Tensor): Log probability values
    
    Returns:
        torch.Tensor: Softmax probability values
    """
    # First convert log probabilities to probabilities using exp
    
    # Convert probabilities to logits
    # logit = log(p / (1-p))
    eps = 1e-12  # small constant to prevent numerical instability
    
    # Apply softmax to get final probabilities
    softmax_probs = probs / (torch.sum(probs) + eps)
    
    return softmax_probs

@app.post("/v1/generateText")
async def generateText(request: Request) -> Response:
    """
    The function `generateText` processes input data, generates text outputs, and calculates scores
    based on the generated text.
    
    :param request: The `request` parameter is of type `Request`, which is used to represent an incoming
    HTTP request. In this case, it seems like you are expecting a JSON payload in the request body,
    which you are then extracting and processing in your `generateText` function. The function processes
    the JSON payload
    :type request: Request
    :return: The function `generateText` is returning a JSON response containing the "result" key, which
    holds a 2D list of normalized scores calculated based on the input chat data. The normalized scores
    are computed by processing the outputs generated by the `llm.chat` function and comparing them with
    the dummy tokens obtained from the chat data. The function performs various memory management
    operations before returning the final result in
    """
    request_dict = await request.json()
    # In batch process in reward_models.py, chat_1: chat with "+", chat_2: chat with "ки" => argument passing
    chat_origin = request_dict.pop("chat") 
    outputs = llm.chat(chat_origin, sampling_params)
    result = []
    for output in outputs:
        prompt_logprobs = output.prompt_logprobs[-6]
        # Since logprob only provides a list of 10, there is no guarantee that there are both good tokens and bad tokens, so divide them into cases. 
        good_score = torch.exp(torch.tensor(prompt_logprobs[plus_tag_id].logprob)).item() if plus_tag_id in prompt_logprobs else 0.0
        bad_score = torch.exp(torch.tensor(prompt_logprobs[minus_tag_id].logprob)).item() if minus_tag_id in prompt_logprobs else 0.0
        norm = torch.nan_to_num(softmax_from_logprob(torch.tensor([good_score, bad_score]))[0]).item() # Good score
        result.append(norm) # 1D list
    
    free_memory()
    print(result)
    ret = {"result": result}
    # del outputs
    # # Randomness in memory management (Time consume down)
    # if random.random() < 0.1 : 
    #     print("Clean!")
    #     gc.collect()
    #     for i in range(num_gpus) :
    #         with torch.cuda.device(f'cuda:{i}') :
    #             torch.cuda.empty_cache()
    #             # Reset CUDA device to fully clear memory
    #             torch.cuda.reset_peak_memory_stats()
    #             torch.cuda.synchronize()  # Wait for all streams on the current device
    return JSONResponse(ret)

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
# tokenizer = llm.get_tokenizer()
# [good_token_id, bad_token_id, step_tag_id] = tokenizer.encode(f"{good_token} {bad_token} {step_tag}")[1:] # [648, 387]

# @app.post("/v1/generateText")
# async def generateText(request: Request) -> Response:
#     request_dict = await request.json()
#     prompt = request_dict.pop("prompts")
#     outputs = llm.generate(prompt, sampling_params)
#     results = []
#     for output in outputs:
#         result_tmp = []
#         prompt_logprobs = output.prompt_logprobs
#         all_tokens = output.prompt_token_ids
#         tag_token_index = [i for i, token in enumerate(all_tokens) if token == step_tag_id]
#         for token_index in tag_token_index:
#             logprobs = prompt_logprobs[token_index]
#             good_score = 0
#             bad_score = 0
#             if good_token_id in logprobs:
#                 good_score = logprobs[good_token_id].logprob
#             if bad_token_id in logprobs:
#                 bad_score = logprobs[bad_token_id].logprob
                
#             normalized_good_score = torch.softmax(torch.tensor([good_score, bad_score]).float(), dim=0)[0].item()
#             result_tmp.append(normalized_good_score)
#         results.append(result_tmp)
#     ret = {"result": results}
#     del outputs, results
#     torch.cuda.empty_cache()
#     # Reset CUDA device to fully clear memory
#     torch.cuda.reset_peak_memory_stats()
#     torch.cuda.synchronize()  # Wait for all streams on the current device
    
#     return JSONResponse(ret)

# if __name__ == "__main__":
#     uvicorn.run(app, host="0.0.0.0", port=8000)