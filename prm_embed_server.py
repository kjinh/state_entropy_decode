import os, socket
from fastapi import FastAPI, Request
from fastapi.responses import Response, JSONResponse
from fastapi import FastAPI
from vllm import LLM, SamplingParams
import uvicorn
import gc
import torch
import random

node_ip = socket.gethostbyname(socket.gethostname())
num_gpus = torch.cuda.device_count()
app = FastAPI()

ip_path = "/home/mjmps0725/state_entropy_decode/src/sal/embed_ip.txt"
with open(ip_path, "w") as f:
    f.write(node_ip)
print(f"prm embedding server running on: {node_ip}")

llm_embeds = LLM(
    model='RLHFlow/Llama3.1-8B-PRM-Deepseek-Data',
    tensor_parallel_size=num_gpus,
    gpu_memory_utilization=0.55,
    task="embed"
)

@app.post("/v1/embeds")
async def embeds(request: Request) -> Response:
    """
    The function `generateText` processes input data, generates text outputs, and calculates scores
    based on the generated text.
    
    :param request: The `request` parameter is of type `Request`, which is used to represent an incoming
    HTTP request. In this case, it seems like you are expecting a JSON payload in the request body,
    which you are then extracting and processing in your `generateText` function. The function processes
    the JSON payload
    :type request: Request
    :return: The function `generateText` is returning a JSON response containing the "result" key, which
    holds a 2D list of normalized scores calculated based on the input chat data. The normalized scores
    are computed by processing the outputs generated by the `llm.chat` function and comparing them with
    the dummy tokens obtained from the chat data. The function performs various memory management
    operations before returning the final result in
    """
    request_dict = await request.json()
    prompts = request_dict.pop("prompts") 
    # print(prompts, output_tmp)
    outputs = [output.outputs.embedding for output in llm_embeds.embed(prompts)]
    ret = {"result": outputs} # 1D List
    del outputs
    if random.random() < 0.1 : 
        print("Clean!")
        gc.collect()
        for i in range(num_gpus) :
            with torch.cuda.device(f'cuda:{i}') :
                torch.cuda.empty_cache()
                # Reset CUDA device to fully clear memory
                torch.cuda.reset_peak_memory_stats()
                torch.cuda.synchronize()  # Wait for all streams on the current device
    return JSONResponse(ret)

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=1200)